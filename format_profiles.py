"""
Aggregates nvprof profile data into one csv file.
Also has capability to validate nvprof success and class balance.
"""

import json
from typing import Mapping, Tuple, Union
from pathlib import Path
import pandas as pd
import numpy as np
from collect_profiles import check_profile
from get_model import name_to_family


def add_model_family(df: pd.DataFrame) -> pd.DataFrame:
    """
    Adds a 'model_family' column to the dataframe based on the 'model' column.

    Example:
    wide_resnet50   -> resnet
    vgg13_bn        -> vgg
    densenet201     -> densenet

    :param df: the dataframe to add the column to.  Must have a column called 'model'
    :return: the original dataframe with the new column.
    """

    def label_family(row):
        return name_to_family[row["model"]]

    df["model_family"] = df.apply(label_family, axis=1)
    return df


def check_for_nans(profile_csv, gpu=0) -> list[str]:
    """Return a list of columns with NaNs in the supplied profile."""

    # aggregate gpu data first:
    skiprows = 3
    with open(profile_csv) as f:
        for i, line in enumerate(f):
            if line == "\n":
                break
    nrows = i - skiprows - 1

    df = pd.read_csv(profile_csv, header=0, skiprows=skiprows, nrows=nrows)
    df = df.drop(0)
    null_cols = df.columns[df.isna().any()].tolist()

    # system data
    skiprows = i + 2
    df = pd.read_csv(profile_csv, header=0, skiprows=skiprows, nrows=5 * (gpu + 1))
    null_system_cols = df.columns[df.isna().any()].tolist()

    null_cols.extend(null_system_cols)

    return null_cols


def parse_one_aggregate_profile(csv_file=None, example=False, nrows=None, skiprows=3):
    """
    Takes a csv generated by nvprof with aggregate mode on, and returns a pandas series where the
    axis labels are the gpu activity/api call name and the metrics associated with it (aggregate time,
    percent time, # of calls, avg time, min time, max time).

    :param csv_file: the csv file
    :param example: boolean to use an example profile.  If true, csv_file is not needed.
    :param nrows: number of rows in the csv, for aggregate profiles.
    :param skiprows: number of rows to skip at the beginning of the csv, for aggregate profiles, default 3
    :return: a pandas series
    """

    if nrows is None:
        with open(csv_file) as f:
            for i, line in enumerate(f):
                if line == "\n":
                    break
        nrows = i-skiprows

    if example:
        csv_file = Path.cwd() / "debug_profiles" / "resnet" / "resnet750691.csv"
    elif not csv_file:
        raise ValueError("csv_file must be provided if example is false.")

    if not csv_file.exists():
        raise ValueError(f"File {csv_file} does not exist")

    gpu_columns={
        "Type": "type",
        "Time": "time_ms",
        "Time(%)": "time_percent",
        "Calls": "num_calls",
        "Avg": "avg_us",
        "Min": "min_us",
        "Max": "max_ms",
        "Name": "name"
    }

    gpu_prof = pd.read_csv(csv_file, header=0, skiprows=skiprows, nrows=nrows)
    gpu_prof = gpu_prof.rename(columns=gpu_columns)
    gpu_prof = gpu_prof.drop(0)

    attribute_cols = ["time_percent", "time_ms", "num_calls", "avg_us", "min_us", "max_ms"]

    result = gpu_prof.apply(
        lambda row: retrieve_row_attrs(row, name_col="name", attribute_cols=attribute_cols),
        axis=1
    )    # results in sparse dataframe
    result = result.backfill()      # put all of the information in the first row

    return result.iloc[0]


def retrieve_row_attrs(row, name_col, attribute_cols):
    """
    Takes 1 row of the gpu attributes such as

    type            time_percent    time_ms     num_calls   avg_us  min_us  max_ms      name
    GPU activities	88.005407	    38.058423	125	        304.467	0.864	13.759156	[CUDA memcpy HtoD]

    and returns a new Series with columns corresponding to the name.

    Example: calling this function on the row above with

    attribute_cols = ["time_percent", "time_ms", "num_calls", "avg_us", "min_us", "max_ms"]
    and
    name_col = "name"
    yeilds

    time_percent_[CUDA memcpy HtoD]     time_ms_[CUDA memcpy HtoD]  ... max_ms_[CUDA memcpy HtoD]
    88.005407                           38.058423                   ... 13.759156
    """

    return pd.Series({f"{attribute}_{row[name_col]}": float(row[attribute]) for attribute in attribute_cols})


def parse_one_system_profile(csv_file=None, example=False, nrows=5, skiprows=None, gpu=0):

    """
    Takes a csv generated by nvprof with aggregate mode on, and returns a pandas series where the
    axis labels are the system signals (clock, memory clock, temp, power, and fan) and the metrics
    associated with it (avg, min, max).

    :param csv_file: the csv file
    :param example: boolean to use an example profile.  If true, csv_file is not needed.
    :param nrows: number of rows in the csv, for aggregate profiles, default is 5
    :param skiprows: number of rows to skip at the beginning of the csv, for aggregate profiles, default 61
                    because the gpu profile activity comes in the first 61 rows
    :param gpu: the number of the gpu used for profiling (nvprof automatically collects system information
                on all gpus in the system)
    :return: a pandas series
    """
    if skiprows is None:
        with open(csv_file) as f:
            for i, line in enumerate(f):
                if line == "\n":
                    break
        skiprows = i + 2    # one blank line and one line with ==System profile result

    if example:
        csv_file = Path.cwd() / "debug_profiles" / "resnet" / "resnet750691.csv"
    elif not csv_file:
        raise ValueError("csv_file must be provided if example is false.")

    if not csv_file.exists():
        raise ValueError(f"File {csv_file} does not exist")

    system_columns = {
        "Device": "device",
        "Count": "count",
        "Avg": "avg",
        "Min": "min",
        "Max": "max",
        "Unnamed: 0": "signal"
    }

    system_prof = pd.read_csv(csv_file, header=0, skiprows=skiprows, nrows=nrows*(gpu+1))
    system_prof = system_prof.rename(columns=system_columns)
    system_prof["signal"] = system_prof["signal"].apply(
        lambda x: x.lower().replace(" ", "_")
    )   # format signal names

    if gpu > 0:
        # drop rows for other gpus
        system_prof = system_prof.drop(list(range(gpu*5)))

    attribute_cols = ["avg", "min", "max"]

    result = system_prof.apply(
        lambda row: retrieve_row_attrs(row, name_col="signal", attribute_cols=attribute_cols),
        axis=1
    )    # results in sparse dataframe
    result = result.backfill()      # put all of the information in the first row
    return result.iloc[0]


def parse_one_profile(csv_file=None, example=False, gpu=0):
    """
    Parse the gpu attributes and system attributes from a csv file from nvprof and return a pandas Series.

    :param csv_file: the csv filename.
    :param example: boolean indicating whether or not to use an example profile.  If true, csv_file is ignored.
    :param gpu: the gpu that the profile was run on.
    :return: a pandas Series
    """
    gpu_prof = parse_one_aggregate_profile(csv_file, example=example)
    system_prof = parse_one_system_profile(csv_file, example=example, gpu=gpu)
    return gpu_prof.append(system_prof)


def parse_all_profiles(folder="profiles", save_filename=None, gpu=0, verbose=True) -> None:
    """
    Parses all of the profiles under the folder into one dataframe saved as a csv in the folder.

    The folder, under cwd/profiles, is organized by subfolder according to model architecture.
    Model architecture and the filename are added as columns to the csv.

    :param folder: the folder containing subfolders by model architecture, which contain profiles,
                        such as ./profiles/<folder>/resnet/resnet12345.csv
    :param save_filename: the filename of the combined csv to save.
    :param gpu: the gpu that the profile was run on.
    :param verbose: print messages.
    :return: None, just saves a csv file.
    """

    # validate that no profiles are corrupt and that there is a class balance
    validate_all(folder)

    root_folder = Path.cwd() / "profiles" / folder
    if not root_folder.exists():
        raise FileNotFoundError(f"Folder {root_folder} does not exist.")

    combined = pd.DataFrame()

    for subdir in [x for x in root_folder.iterdir() if x.is_dir()]:
        model = subdir.name
        if verbose:
            print(f"Parsing profiles for {model}")
        for csv_profile in [x for x in subdir.iterdir()]:
            file = csv_profile.name
            if verbose:
                print(f"\t{file}")
            prof = parse_one_profile(csv_file=csv_profile, gpu=gpu)
            prof = pd.Series({"file" : file, "model": model}).append(prof)
            combined = combined.append(prof, ignore_index=True)

    if save_filename is None:
        save_filename = "aggregated.csv"

    save_path = root_folder / save_filename

    combined = add_model_family(combined)
    combined.to_csv(save_path, index=False)
    return


def validate_all(folder: Path) -> None:
    """
    Validates 3 things:

    (1) that nvprof did not fail on any profile.
    (2) that there are no NaNs in the profiles.
    (3) that there is the same number of profiles per class.

    If any check fails, an error is raised. Also, the user will have the option to remove profiles
    based on a response to a question in the console.

    :param folder: the root folder which has subfolders organized by class (model architecture)
    :return: None
    """

    # check that all profiles are valid
    valid, _ = validate_nvprof(folder, remove=False)
    if not valid:
        response = input(
            "\n\n\nThere are invalid profiles.  Enter 'yes' to delete them, anything "
            "else to keep them.  An error will be raised either way.  This error will "
            "continue occuring until they are moved or deleted.")
        if response.lower() == 'yes':
            _ = validate_nvprof(folder, remove=True)
        raise ValueError("Invalid profiles, fix before aggregating.")

    no_nans, _ = validate_nans(folder, remove=False)
    if not no_nans:
        response = input(
            "\n\n\nThere are profiles with NaNs.  Enter 'yes' to delete them, anything "
            "else to keep them.  An error will be raised either way.  This error will "
            "continue occuring until they are fixed or deleted."
        )
        if response.lower() == 'yes':
            _ = validate_nans(folder, remove=True)
        raise ValueError("Profiles have NaNs, fix before aggregating.")

    # check that classes are balanced
    balanced = validate_class_balance(folder, remove=False)
    if not balanced:
        response = input(
            "\n\n\nThere is a class imbalance. Enter 'yes' to delete extra profiles, "
            "enter anything else to keep them.  An error will be raised either way. "
            "This error will continue occuring until the classes are balanced.")
        if response.lower() == 'yes':
            _ = validate_class_balance(folder, remove=True)
        raise ValueError("Class imbalance, fix before aggregating.")


def validate_nvprof(folder: Path, remove: bool=False) -> Tuple[bool, Mapping[str, Mapping[str, Union[int, list[str]]]]]:
    """
    Checks all the profiles under ./profiles/<folder> to see if nvprof failed and lists them, optionally removing them.

    :param folder: the folder containing subfolders by model architecture, which contain profiles,
                        such as ./profiles/<folder>/resnet/resnet12345.csv
    :param remove: boolean whether or not to remove the files
    :return: a tuple of (boolean indicating whether there were any invalid profiles,
                    a dictionary of how many invalid profiles there are by model, with the file names)
    """

    print("Checking profile validity ... ")

    root_folder = Path.cwd() / "profiles" / folder
    if not root_folder.exists():
        raise FileNotFoundError(f"Folder {root_folder} does not exist.")

    all_valid = True
    invalid_profiles = {}

    for subdir in [x for x in root_folder.iterdir() if x.is_dir()]:
        model = subdir.name
        invalid_profiles[model] = {"num_invalid": 0, "invalid_profiles": []}
        print(f"Parsing profiles for {model}")
        for csv_profile in [x for x in subdir.iterdir()]:
            file = csv_profile.name
            valid = check_profile(csv_profile)
            if not valid:
                all_valid = False
                print(f"\t{file} is invalid!")
                invalid_profiles[model]["num_invalid"] += 1
                invalid_profiles[model]["invalid_profiles"].append(str(csv_profile))
                if remove:
                    csv_profile.unlink()

    if all_valid:
        print("All profiles valid!\n\n")
    else:
        print("Invalid profiles!")
        print(json.dumps(invalid_profiles, indent=4))
    return all_valid, invalid_profiles


def validate_class_balance(folder: Path, remove: bool=False) -> bool:
    """
    Checks all the profiles under ./profiles/<folder> to see if there is a class balance, optionally removing extras.

    :param folder: the folder containing subfolders by model architecture, which contain profiles,
                        such as ./profiles/<folder>/resnet/resnet12345.csv
    :param remove: boolean whether or not to remove the files
    :return: boolean indicating whether there is a class balance
    """

    print("Checking class balance ... ")

    root_folder = Path.cwd() / "profiles" / folder
    if not root_folder.exists():
        raise FileNotFoundError(f"Folder {root_folder} does not exist.")

    profiles = {}

    for subdir in [x for x in root_folder.iterdir() if x.is_dir()]:
        model = subdir.name
        profiles[model] = {"num": 0, "profiles": []}
        print(f"Parsing profiles for {model}")
        for csv_profile in [x for x in subdir.iterdir()]:
            profiles[model]["num"] += 1
            profiles[model]["profiles"].append(csv_profile)

    model_counts = [profiles[model]["num"] for model in profiles]
    balance = len(model_counts) == model_counts.count(model_counts[0])

    if balance:
        print("Classes are balanced!\n\n")
    else:
        print("Classes are imbalanced!")
        print(json.dumps({model: f"{profiles[model]['num']} profiles" for model in profiles}, indent=4))

    if remove:
        keep = min(model_counts)
        for model in profiles:
            count = profiles[model]["num"]
            need_to_remove = count - keep
            if need_to_remove > 0:
                for i in range(need_to_remove):
                    file = profiles[model]["profiles"][i]
                    print(f"Removing {file}")
                    file.unlink()
    return balance


def validate_nans(folder: Path, remove: bool=False) -> Tuple[bool, Mapping[str, Mapping[str, Union[int, list[str]]]]]:
    """
    Checks all the profiles under ./profiles/<folder> to see if they include NaNs and lists them.

    :param folder: the folder containing subfolders by model architecture, which contain profiles,
                        such as ./profiles/<folder>/resnet/resnet12345.csv
    :param remove: boolean whether or not to remove the files with NaNs
    :return: a tuple of (boolean indicating whether there were any profiles with NaNs,
                    a dictionary of profiles with NaNs by model, with the file names)
    """

    print("Checking profiles for NaNs ... ")

    root_folder = Path.cwd() / "profiles" / folder
    if not root_folder.exists():
        raise FileNotFoundError(f"Folder {root_folder} does not exist.")

    no_nans = True
    profiles_with_nan = {}

    for subdir in [x for x in root_folder.iterdir() if x.is_dir()]:
        model = subdir.name
        profiles_with_nan[model] = {"num_with_nan": 0, "profiles": []}
        print(f"Parsing profiles for {model}")
        for csv_profile in [x for x in subdir.iterdir()]:
            file = csv_profile.name
            cols = check_for_nans(csv_profile)
            if len(cols) > 0:
                no_nans = False
                print(f"\t{file} has NaNs in columns {cols}")
                profiles_with_nan[model]["num_with_nan"] += 1
                profiles_with_nan[model]["profiles"].append(str(csv_profile))
                if remove:
                    csv_profile.unlink()

    if no_nans:
        print("No NaNs in any profiles!\n\n")
    else:
        print("NaNs found in profiles!")
        print(json.dumps(profiles_with_nan, indent=4))
    return no_nans, profiles_with_nan


def read_csv(folder: Path=None, gpu: int=0) -> pd.DataFrame:
    """
    Reads the aggregated csv data from the folder.  If the aggregated csv does not exist, creates it.

    :param folder: the folder name under ./profiles/<folder> where the profiles are stored.
    :return: a pandas dataframe
    """
    if not folder:
        folder = "debug_profiles"

    folder_path = Path.cwd() / "profiles" / folder
    aggregated_csv_file = folder_path / "aggregated.csv"
    if not aggregated_csv_file.exists():
        parse_all_profiles(folder, gpu=gpu)
    return pd.read_csv(aggregated_csv_file)


if __name__ == '__main__':
    # a = parse_all_profiles("debug_2")
    # validate_nans("zero_noexe")
    parse_all_profiles("zero_noexe_lots_models")
    # validate_nvprof("zero_noexe_lots_models")
    # validate_class_balance("zero_noexe_lots_models")
    exit(0)
